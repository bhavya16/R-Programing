Data Preprocessing Steps
========================

> tweets = read.csv("tweets.csv",stringsAsFactors=FALSE)
> str(tweets)
'data.frame':   1181 obs. of  2 variables:
 $ Tweet: chr  "I have to say, Apple has by far the best customer care service I have ever received! @Apple @AppStore" "iOS 7 is so fricking smooth & beautiful!! #ThanxApple @Apple" "LOVE U @APPLE" "Thank you @apple, loving my new iPhone 5S!!!!!  #apple #iphone5S pic.twitter.com/XmHJCU4pcb" ...
 $ Avg  : num  2 2 1.8 1.8 1.8 1.8 1.8 1.6 1.6 1.6 ...
> tweets$Negative = as.factor(tweets$Avg <= -1)
> table(tweets$Negative)

FALSE  TRUE 
  999   182 
> install.packages("tm")
Installing package into ‘C:/Users/Bhavya/Documents/R/win-library/3.1’
(as ‘lib’ is unspecified)
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/3.1/tm_0.6-1.zip'
Content type 'application/zip' length 709849 bytes (693 KB)
opened URL
downloaded 693 KB

package ‘tm’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
        C:\Users\Bhavya\AppData\Local\Temp\RtmpuYoh3C\downloaded_packages
> library(tm)
Loading required package: NLP
> install.packages("SnowballC")
Installing package into ‘C:/Users/Bhavya/Documents/R/win-library/3.1’
(as ‘lib’ is unspecified)
trying URL 'http://lib.stat.cmu.edu/R/CRAN/bin/windows/contrib/3.1/SnowballC_0.5.1.zip'
Content type 'application/zip' length 3076457 bytes (2.9 MB)
opened URL
downloaded 2.9 MB

package ‘SnowballC’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
        C:\Users\Bhavya\AppData\Local\Temp\RtmpuYoh3C\downloaded_packages
> LIBRARY(SnowballC)
Error: could not find function "LIBRARY"
> library(SnowballC)
> corpus = Corpus(VectorSource(tweets$Tweet))
> corpus
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 1181
> corpus[[1]]
<<PlainTextDocument>>
Metadata:  7
Content:  chars: 101
> corpus = tm_map(corpus , tolower)
> corpus[[1]]
[1] "i have to say, apple has by far the best customer care service i have ever received! @apple @appstore"
> corpus = tm_map(corpus , removePunctuation)
> corpus[[1]]
[1] "i have to say apple has by far the best customer care service i have ever received apple appstore"
> stopwords("english")[1:10])
Error: unexpected ')' in "stopwords("english")[1:10])"
> stopwords("english")[1:10]
 [1] "i"         "me"        "my"        "myself"    "we"        "our"      
 [7] "ours"      "ourselves" "you"       "your"     
> corpus = tm_map(corpus , removeWords, c("apple",stopwrds("english")))
Error in sort(words, decreasing = TRUE) : 
  could not find function "stopwrds"
> corpus = tm_map(corpus , removeWords, c("apple",stopwords("english")))
> corpus[[1]]
[1] "   say    far  best customer care service   ever received  appstore"
> corpus = tm_map(corpus , stemDocument)
> corpus[[1]]
[1] "   say    far  best customer care service   ever received  appstor"



> frequencies = DocumentTermMatrix(corpus)
> frequencies
<<DocumentTermMatrix (documents: 1181, terms: 3289)>>
Non-/sparse entries: 8980/3875329
Sparsity           : 100%
Maximal term length: 115
Weighting          : term frequency (tf)
> inspect(frequencies[1000:1005,505:515])
<<DocumentTermMatrix (documents: 6, terms: 11)>>
Non-/sparse entries: 1/65
Sparsity           : 98%
Maximal term length: 9
Weighting          : term frequency (tf)

              Terms
Docs           cheapen cheaper check cheep cheer cheerio cherylcol chief
  character(0)       0       0     0     0     0       0         0     0
  character(0)       0       0     0     0     0       0         0     0
  character(0)       0       0     0     0     0       0         0     0
  character(0)       0       0     0     0     0       0         0     0
  character(0)       0       0     0     0     0       0         0     0
  character(0)       0       0     0     0     1       0         0     0
              Terms
Docs           chiiiiqu child children
  character(0)        0     0        0
  character(0)        0     0        0
  character(0)        0     0        0
  character(0)        0     0        0
  character(0)        0     0        0
  character(0)        0     0        0
> fndFreqTerms(frequencies, lowfreq20)
Error: could not find function "fndFreqTerms"
> fndFreqTerms(frequencies, lowfreq=20)
Error: could not find function "fndFreqTerms"
> findFreqTerms(frequencies, lowfreq=20)
 [1] "android"              "anyon"                "app"                  "appl"                
 [5] "back"                 "batteri"              "better"               "buy"                 
 [9] "can"                  "cant"                 "come"                 "dont"                
[13] "fingerprint"          "freak"                "get"                  "googl"               
[17] "ios7"                 "ipad"                 "iphon"                "iphone5"             
[21] "iphone5c"             "ipod"                 "ipodplayerpromo"      "itun"                
[25] "just"                 "like"                 "lol"                  "look"                
[29] "love"                 "make"                 "market"               "microsoft"           
[33] "need"                 "new"                  "now"                  "one"                 
[37] "phone"                "pleas"                "promo"                "promoipodplayerpromo"
[41] "realli"               "releas"               "samsung"              "say"                 
[45] "store"                "thank"                "think"                "time"                
[49] "twitter"              "updat"                "use"                  "via"                 
[53] "want"                 "well"                 "will"                 "work"                
> sparse = removeSparseTerms(frequencies, 0.995)
> sparse
<<DocumentTermMatrix (documents: 1181, terms: 309)>>
Non-/sparse entries: 4669/360260
Sparsity           : 99%
Maximal term length: 20
Weighting          : term frequency (tf)
> tweetsSparse = as.data.frame(as.matrix(sparse))
> colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
> tweetsSparseNegative = tweets$Negative
> library(caTools)
> set.seed(123)
> split = sample.split(tweetsSparse$Negative, SplitRatio=0.7)
Error in sample.split(tweetsSparse$Negative, SplitRatio = 0.7) : 
  Error in sample.split: 'SplitRatio' parameter has to be i [0, 1] range or [1, length(Y)] range
> tweetsSparse$Negative= tweets$Negative
> library(caTools)
> set.seed(123)
> split = sample.split(tweetsSparse$Negative, SplitRatio=0.7)
> trainSparse= subset(tweetsSparse,split == TRUE)
> testSparse= subset(tweetsSparse,split == FALSE)
> findFreqTerms(frequencies, lowfreq >=100)
Error in stopifnot(inherits(x, c("DocumentTermMatrix", "TermDocumentMatrix")),  : 
  object 'lowfreq' not found
> findFreqTerms(frequencies, lowfreq=100)
[1] "iphon" "itun"  "new"  


> library(rpart)
> library(rpart.plot)
> tweetCART =rpart(Negative ~ .,data=trainSparse, method ="class")
> prp(tweetCART)
> predictCART=predict(tweetCART, newdata = testSparse, type = "class")
> table(testSparse$Negative, predictCART)
       predictCART
        FALSE TRUE
  FALSE   294    6
  TRUE     37   18
> (294+18)/294+18+37+6)
Error: unexpected ')' in "(294+18)/294+18+37+6)"
> (294+18)/(294+18+37+6)
[1] 0.8788732
> (294+6)/(294+18+37+6)
[1] 0.8450704
> library(randomForest)
randomForest 4.6-10
Type rfNews() to see new features/changes/bug fixes.
> set.seed(123)
> tweetRF = randomForest(Negative ~ ., data = trainSparse)
> predictRF = predict(tweetRF, newdata = testSparse)
> table(testSparse$Negative,predictRF)
       predictRF
        FALSE TRUE
  FALSE   293    7
  TRUE     34   21
> (293+21)/(293+21+34+7)
[1] 0.884507
> 
 
================================================================================================================================================================================================================================================================

ENERGY BIDS EXAMPLE TO ILLUSTRATE TEXT ANALYTICS
-------------------------------------------------

> emails = read.csv("energy_bids.csv", stringsAsFactors=FALSE)  ====In text analytics we use one extra argument stringsAsFactor=FALSE

> str(emails)
'data.frame':   855 obs. of  2 variables:
 $ email     : chr  "North America's integrated electricity market requires 

> emails$email[1]
[1] "North America's integrated electricity market requires cooperation on environmental policies Commission for Environmental Cooperation releases working paper on North America's electricity market Montreal, 27 November 2001 -- The North American Commission for Environmental Cooperation (CEC) is releasing a working paper highlighting the trend towards increasing trade, competition and cross-border investment in electricity between Canada, Mexico and the United States. It is hoped that the working paper, Environmental Challenges and Opportunities in the Evolving North American Electricity Market, will stimulate public discussion around a CEC symposium of the same title about the need to coordinate environmental policies trinationally as a North America-wide electricity market develops. The CEC symposium will take place in San Diego on 29-30 November, and will bring together leading experts from industry, academia, NGOs and the governments of Canada, Mexico and the United States to consider the impact of the evolving continental electricity market on human health and the environment. \"Our goal [with the working paper and the symposium] is to highlight key environmental issues that must be addressed as the electricity markets in North America become more and more integrated,\" said Janine Ferretti, executive director of the CEC. \"We want to stimulate discussion around the important policy questions being raised so that countries can cooperate in their approach to energy and the environment.\" The CEC, an international organization created under an environmental side agreement to NAFTA known as the North American Agreement on Environmental Cooperation, was established to address regional environmental concerns, help prevent potential trade and environmental conflicts, and promote the effective enforcement of environmental law. The CEC Secretariat believes that greater North American cooperation on environmental policies regarding the continental electricity market is necessary to: *  protect air quality and mitigate climate change, *  minimize the possibility of environment-based trade disputes, *  ensure a dependable supply of reasonably priced electricity across North America *  avoid creation of pollution havens, and *  ensure local and national environmental measures remain effective. The Changing Market The working paper profiles the rapid changing North American electricity market. For example, in 2001, the US is projected to export 13.1 thousand gigawatt-hours (GWh) of electricity to Canada and Mexico. By 2007, this number is projected to grow to 16.9 thousand GWh of electricity. \"Over the past few decades, the North American electricity market has developed into a complex array of cross-border transactions and relationships,\" said Phil Sharp, former US congressman and chairman of the CEC's Electricity Advisory Board. \"We need to achieve this new level of cooperation in our environmental approaches as well.\" The Environmental Profile of the Electricity Sector The electricity sector is the single largest source of nationally reported toxins in the United States and Canada and a large source in Mexico. In the US, the electricity sector emits approximately 25 percent of all NOx emissions, roughly 35 percent of all CO2 emissions, 25 percent of all mercury emissions and almost 70 percent of SO2 emissions. These emissions have a large impact on airsheds, watersheds and migratory species corridors that are often shared between the three North American countries. \"We want to discuss the possible outcomes from greater efforts to coordinate federal, state or provincial environmental laws and policies that relate to the electricity sector,\" said Ferretti. \"How can we develop more compatible environmental approaches to help make domestic environmental policies more effective?\" The Effects of an Integrated Electricity Market One key issue raised in the paper is the effect of market integration on the competitiveness of particular fuels such as coal, natural gas or renewables. Fuel choice largely determines environmental impacts from a specific facility, along with pollution control technologies, performance standards and regulations. The paper highlights other impacts of a highly competitive market as well. For example, concerns about so called \"pollution havens\" arise when significant differences in environmental laws or enforcement practices induce power companies to locate their operations in jurisdictions with lower standards. \"The CEC Secretariat is exploring what additional environmental policies will work in this restructured market and how these policies can be adapted to ensure that they enhance competitiveness and benefit the entire region,\" said Sharp. Because trade rules and policy measures directly influence the variables that drive a successfully integrated North American electricity market, the working paper also addresses fuel choice, technology, pollution control strategies and subsidies. The CEC will use the information gathered during the discussion period to develop a final report that will be submitted to the Council in early 2002. For more information or to view the live video webcast of the symposium, please go to: http://www.cec.org/electricity. You may download the working paper and other supporting documents from: http://www.cec.org/programs_projects/other_initiatives/electricity/docs.cfm?varlan=english. Commission for Environmental Cooperation 393, rue St-Jacques Ouest, Bureau 200 MontrÃ©al (QuÃ©bec) Canada H2Y 1N9 Tel: (514) 350-4300; Fax: (514) 350-4314 E-mail: info@ccemtl.org ***********"
> strwrap(emails$email[1]) =======to avoid the contents to be in single line (difficult to read)we use strwrap function
  [1] "North America's integrated electricity market requires cooperation on"                      
 [2] "environmental policies Commission for Environmental Cooperation"                            
 [3] "releases working paper on North America's electricity market Montreal,"                     
 [4] "27 November 2001 -- The North American Commission for Environmental"                        
 [5] "Cooperation (CEC) is releasing a working paper highlighting the trend"                      
 [6] "towards increasing trade, competition and cross-border investment in"            .....................................           
 ..................................         
[82] "(514) 350-4314 E-mail: info@ccemtl.org ***********"      

PREPROCESSING STEPS STARTS HERE
------------------------------------------------------------------------------------                                    
> install.packages("tm")     ========= we need to install "tm" package to perform the pre-processing steps

Installing package into ‘C:/Users/Bhavya/Documents/R/win-library/3.1’
(as ‘lib’ is unspecified)
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://ftp.ussg.iu.edu/CRAN/bin/windows/contrib/3.1/tm_0.6-2.zip'
Content type 'application/zip' length 710884 bytes (694 KB)
opened URL
downloaded 694 KB

package ‘tm’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
        C:\Users\Bhavya\AppData\Local\Temp\RtmpeWjo3b\downloaded_packages

> library(tm)
Loading required package: NLP

> corpus = Corpus(VectorSource(emails$email)) ====we will construct a variable corpus using Corpus and VectorSource function

> strwrap(corpus[[1]]) =====it will show the first email of corpus variable in wrap format
 [1] "North America's integrated electricity market requires cooperation on"                      
 [2] "environmental policies Commission for Environmental Cooperation"                            
 [3] "releases working paper on North America's electricity market Montreal,"                     
 [4] "27 November 2001 -- The North American Commission for Environmental"                        
 [5] "Cooperation (CEC) is releasing a working paper highlighting the trend"                      
 [6] "towards increasing trade, competition and cross-border investment in"            .....................................           
 ..................................         
[82] "(514) 350-4314 E-mail: info@ccemtl.org ***********"   

>corpus = tm_map(corpus, tolower) =======covert the corpus to lowercase using tm_map function

>corpus = tm_map(corpus, PlainTextDocument)  =======converts all documents in the corpus to the PlainTextDocument type

>corpus = tm_map(corpus, removePunctuation) =====remove punctuation from the corpus

>corpus = tm_map(corpus, removeWords, stopwords("english"))  ===remove stop words with removeWords function
>corpus = tm_map(corpus, stemDocument) ====stem the document


> strwrap(corpus[[1]]) ====== shows the first email in the corpus

PREPROCESSING STEPS ENDS HERE AND NOW THE EMAILS IN THE CORPUS ARE READY FOR MACHINE LEARNING ALGORITHMS.
-----------------------------------------------------------------------------------

> dtm = DocumentTermMatrix(corpus) ====== creating variable called dtm to build document-term matrix for our corpus.

> dtm =====to get summary statistics about document-term matrix
<<DocumentTermMatrix (documents: 855, terms: 22164)>>
Non-/sparse entries: 102863/18847357
Sparsity           : 99%
Maximal term length: 156
Weighting          : term frequency (tf)

> dtm = removeSparseTerms(dtm, 0.97)======since we have so many terms(22164)in the emails we want to remove the terms that dont appear too often in our dataset,so we will use "removeSparseTerms" function and also we need to specify the sparsity,so here we will say that we'll remove any terms that doesnot appear in atleast 3% of the document.To do that we will pass 0.97 to removeSparseTerms.

> dtm ==== now we an c that the terms are reduced to 788.
<<DocumentTermMatrix (documents: 855, terms: 788)>>
Non-/sparse entries: 51612/622128
Sparsity           : 92%
Maximal term length: 19
Weighting          : term frequency (tf)

> labeledTerms = as.data.frame(as.matrix(dtm))  ===building a dataframe of the dtm.

> labeledTerms$responsive = emails$responsive ==== We have added the variable responsive in "labeledTerms" vaiable.

> str(labeledTerms)=======so now we can c 789 variables, 788 are of those variables  are the frequencies of various words in the email and the last one is responsive(the outcome variable).

'data.frame':   855 obs. of  789 variables:

labeledTerms   ====This is the dataset we are going to use to build a model

-------------------------------------------------------------------------------

BUILDING A MODEL(using CART)
-----------------------------

> library(caTools)
> set.seed(144)
> split = sample.split(labeledTerms$responsive, 0.7)
> train = subset(labeledTerms, split ==TRUE)
> test = subset(labeledTerms, split ==FALSE)
> library(rpart)
> library(rpart.plot) =====goig to use CART model so we need ti load up "rpart" and "rpart.plot(so that we can plot the outcome)" packages.
>emailCART = rpart(responsive ~., data = train, method ="class")  =====We will create a model calles emailCART using rpart function.
>prp(emailCART)  ===we can plot the emailCART model using prp function

>pred = predict(emailCART, newdata = test)   ===Now that we have trained a model,we need to evaluate it on test set.to get test set predicted probability.
>pred[1:10,] ====to recall the structure of pred,we can look at the first 10 rows and all the columns
                       0          1
character(0)   0.2156863 0.78431373
character(0).1 0.9557522 0.04424779
character(0).2 0.9557522 0.04424779
character(0).3 0.8125000 0.18750000
character(0).4 0.4000000 0.60000000
character(0).5 0.9557522 0.04424779
character(0).6 0.9557522 0.04424779
character(0).7 0.9557522 0.04424779
character(0).8 0.1250000 0.87500000
character(0).9 0.1250000 0.87500000

***0 columns denotes the predictive probability of document being non-responsive***
***1 column denotes the predictive probability of document being responsive***
***so in our case we want to extract the predicted prob of the document being responsive,so we'll createobject called pred.prob.***

> pred.prob = pred[,2]
> table(test$responsive, pred.prob >= 0.5)
   
    FALSE TRUE
  0   195   20
  1    17   25

> (195+25)/(195+25+17+20)  
[1] 0.8560311   ===so the accuracy oin the test set is 85.6%

> (195+20)/(195+25+17+20)   (or) table(test$responsive)
[1] 0.8365759  === accuracy of the baseline model is 83.7%

**so here we can see a small improvement in accuracy using the CART model which is very common case in unbalanced data sets.***

***now lets look at ROC curve to understand the performance of our model at different cut-offs.****

> library(ROCR)
Loading required package: gplots

Attaching package: ‘gplots’

The following object is masked from ‘package:stats’:

    lowess

> predROCR = prediction(pred.prob, test$responsive)
> perfROCR = performance(predROCR, "tpr","fpr")
> plot(perfROCR, colorize = TRUE)
> performance(predROCR, "auc")@y.values 
[[1]]
[1] 0.7936323

****AUC Value is 79.4% which means that our model can differenciate between a randomly selected responsive and non-responsive document about 80%of the time***



============================================================================================================================

example::random

# installing required packages
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
install.packages("wordcloud")
library(wordcloud)

# VISUALIZING TEXT DATA USING WORD CLOUDS
###############################################################################
# PROBLEM 1 - PREPARING THE DATA
# Loading data
tweets = read.csv("tweets.csv")
# Preprocessing the data
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
# Build a document-term matrix out of the corpus
frequencies = DocumentTermMatrix(corpus)
#
allTweets = as.data.frame(as.matrix(frequencies))
str(allTweets)






