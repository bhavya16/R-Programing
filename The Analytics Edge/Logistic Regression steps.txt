LOGISTIC REGRESSION STEPS
===========================
imputed = complete(mice(simple))---------------for filling missing values

load the dataset into R by creating dataframe called "framingham"

framingham = read.csv("framingham.csv")
              

Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)

> library(caTools)
> 
> set.seed(1000)
> split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
> train = subset(framingham, split == TRUE)----create dataframe train
> test = subset(framingham, split == FALSE)----create dataframe test
> str(train)
'data.frame':   2756 obs. of  16 variables:
 $ male           : int  1 1 0 0 0 0 1 0 0 1 ...
 $ age            : int  39 48 61 43 63 45 43 50 43 46 ...
 $ education      : int  4 1 3 2 1 2 1 1 2 1 ...
 $ currentSmoker  : int  0 1 1 0 0 1 1 0 0 1 ...
 $ cigsPerDay     : int  0 20 30 0 0 20 30 0 0 15 ...
 $ BPMeds         : int  0 0 0 0 0 0 0 0 0 0 ...
 $ prevalentStroke: int  0 0 0 0 0 0 0 0 0 0 ...
 $ prevalentHyp   : int  0 0 1 1 0 0 1 0 0 1 ...
 $ diabetes       : int  0 0 0 0 0 0 0 0 0 0 ...
 $ totChol        : int  195 245 225 228 205 313 225 254 247 294 ...
 $ sysBP          : num  106 128 150 180 138 ...
 $ diaBP          : num  70 80 95 110 71 71 107 76 88 94 ...
 $ BMI            : num  27 25.3 28.6 30.3 33.1 ...
 $ heartRate      : int  80 75 65 77 60 79 93 75 72 98 ...
 $ glucose        : int  77 70 103 99 85 78 88 76 61 64 ...
 $ TenYearCHD     : int  0 0 1 0 1 0 0 0 0 0 ...
> str(test)
'data.frame':   1484 obs. of  16 variables:
 $ male           : int  0 0 1 0 1 0 0 1 1 0 ...
 $ age            : int  46 46 52 41 48 38 60 43 37 41 ...
 $ education      : int  2 3 1 3 3 2 1 4 2 2 ...
 $ currentSmoker  : int  0 1 0 0 1 1 0 1 0 1 ...
 $ cigsPerDay     : int  0 23 0 0 10 5 0 43 0 1 ...
 $ BPMeds         : int  0 0 0 1 0 0 0 0 0 0 ...
 $ prevalentStroke: int  0 0 0 0 0 0 0 0 0 0 ...
 $ prevalentHyp   : int  0 0 1 1 1 0 0 0 1 0 ...
 $ diabetes       : int  0 0 0 0 0 0 0 0 0 0 ...
 $ totChol        : int  250 285 260 332 232 195 260 226 225 237 ...
 $ sysBP          : num  121 130 142 124 138 ...
 $ diaBP          : num  81 84 89 88 90 84.5 72.5 85.5 92.5 78 ...
 $ BMI            : num  28.7 23.1 26.4 31.3 22.4 ...
 $ heartRate      : int  95 85 76 65 64 75 65 75 95 75 ...
 $ glucose        : int  76 85 79 84 72 78 NA 75 83 74 ...
 $ TenYearCHD     : int  0 0 0 0 0 0 0 0 0 0 ...

CREATING LOGISTIC MODEL
------------------------

> framinghamLog = glm(TenYearCHD ~ ., data = train, family = binomial)
> summary(framinghamLog)

Call:
glm(formula = TenYearCHD ~ ., family = binomial, data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8487  -0.6007  -0.4257  -0.2842   2.8369  

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)    
(Intercept)     -7.886574   0.890729  -8.854  < 2e-16 ***
male             0.528457   0.135443   3.902 9.55e-05 ***
age              0.062055   0.008343   7.438 1.02e-13 ***
education       -0.058923   0.062430  -0.944  0.34525    
currentSmoker    0.093240   0.194008   0.481  0.63080    
cigsPerDay       0.015008   0.007826   1.918  0.05514 .  
BPMeds           0.311221   0.287408   1.083  0.27887    
prevalentStroke  1.165794   0.571215   2.041  0.04126 *  
prevalentHyp     0.315818   0.171765   1.839  0.06596 .  
diabetes        -0.421494   0.407990  -1.033  0.30156    
totChol          0.003835   0.001377   2.786  0.00533 ** 
sysBP            0.011344   0.004566   2.485  0.01297 *  
diaBP           -0.004740   0.008001  -0.592  0.55353    
BMI              0.010723   0.016157   0.664  0.50689    
heartRate       -0.008099   0.005313  -1.524  0.12739    
glucose          0.008935   0.002836   3.150  0.00163 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2020.7  on 2384  degrees of freedom
Residual deviance: 1792.3  on 2369  degrees of freedom
  (371 observations deleted due to missingness)
AIC: 1824.3

Number of Fisher Scoring iterations: 5

To find the accuracy of the model(framinghamLog) using a threshold of 0.5
-----------------------------------------------------------------------------

> predictTest = predict(framinghamLog, type="response", newdata=test)
> table(test$TenYearCHD, predictTest>0.5)   --------Confusion Matrix
   
    FALSE TRUE
  0  1069    6
  1   187   11

> (1069+11)/(1069+6+187+11)
[1] 0.8483896
So the accuracy of our model is about 84.8%

We need to compare this to the accuracy of a simple baseline
------------------------------------------------------------
 The more frequent outcome in this case is 0,
 so the baseline method would always predict 0 or no CHD.
 This baseline method would get an accuracy of 1069

to find the accuracy of the baseline model
---------------------------------------
> (1069 + 6)/(1069 + 6 + 187 + 11)
[1] 0.8444619
So the baseline model would get an accuracy of about 84.4%

To find AUC
----------------
> library(ROCR)
> ROCRpred = prediction(predictTest, test$TenYearCHD)
> as.numeric(performance(ROCRpred,"auc")@y.values)
[1] 0.7421095
So we have an AUC of about 74% on our test





> eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
> 
> eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
> library(tm)
Loading required package: NLP
> CorpusDescription = Corpus(VectorSource(c(eBayTrain$description, eBayTest$description)))
> CorpusDescription = tm_map(CorpusDescription, content_transformer(tolower), lazy=TRUE)
> 
> CorpusDescription = tm_map(CorpusDescription, PlainTextDocument, lazy=TRUE)
> CorpusDescription = tm_map(CorpusDescription, removePunctuation, lazy=TRUE)
> 
> CorpusDescription = tm_map(CorpusDescription, removeWords, stopwords("english"), lazy=TRUE)
> 
> CorpusDescription = tm_map(CorpusDescription, stemDocument, lazy=TRUE)
> dtm = DocumentTermMatrix(CorpusDescription)
> sparse = removeSparseTerms(dtm, 0.99)
> sparse
<<DocumentTermMatrix (documents: 2659, terms: 72)>>
Non-/sparse entries: 5444/186004
Sparsity           : 97%
Maximal term length: 10
Weighting          : term frequency (tf)
> DescriptionWords = as.data.frame(as.matrix(sparse))
> colnames(DescriptionWords) = make.names(colnames(DescriptionWords))
> DescriptionWordsTrain = head(DescriptionWords, nrow(eBayTrain))
> DescriptionWordsTest = tail(DescriptionWords, nrow(eBayTest))
> DescriptionWordsTrain$sold = eBayTrain$sold
> DescriptionWordsTrain$WordCount = eBayTrain$WordCount
> DescriptionWordsTest$WordCount = eBayTest$WordCount


DescriptionWordsLog1 = glm(sold ~ still + test +use +wear + screen +pleas +perfect + hous + condition +charger + X100, data=DescriptionWordsTrain, family=binomial)
> PredTest1 = predict(DescriptionWordsLog1, newdata=DescriptionWordsTest, type="response")
> MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest1)
> write.csv(MySubmission, "SubmissionDescriptionLog1.csv", row.names=FALSE)